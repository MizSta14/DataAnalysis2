\documentclass[letterpaper, 12pt]{article}


\usepackage{parskip,xspace}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{mathrsfs} 
\usepackage{caption}
\usepackage{xcolor} 
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{ltxtable}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{bm}
\usepackage[]{statrep}
\usepackage{enumerate}
\usepackage{subfigure}
\graphicspath{{eps/}}


\newcommand{\ba}{$$\begin{aligned}}
\newcommand{\ea}{\end{aligned}$$}
\newcommand{\dx}{\mathrm{d}x}






\pagestyle{fancy}
\lhead{Peng Shao 14221765}
\chead{}
\rhead{\bfseries STAT 8320 Spring 2015 Assignment 4}
\renewcommand{\headrulewidth}{0.4 pt}
\setlength{\parindent}{2em}

\begin{document}
\title{STAT 8320 Spring 2015 Assignment 4}
\author{Peng Shao 14221765}
\maketitle
\indent




$\blacktriangleright$ \textbf{1.\quad Solution.} 
(a). Define
\ba
\bm{Y}_i&=(Y_i1,Y_i2)'\\
\bm{\beta}&=(\beta_0,\beta_0)'\\
\bm{b}_i&=(b_{0i},b_{1i})'\\
\bm{W}_i&=\left(\begin{matrix}
1&W_{i1}\\
1&W_{i2}
\end{matrix}\right)\\
\bm{\epsilon}_i&=(\epsilon_{i1},\epsilon_{i2})'\\
\bm{D}&=\left(\begin{matrix}
4 &1\\
1 &2
\end{matrix}\right)\\
\bm{\Sigma}&=\left(\begin{matrix}
\sigma^2\\
&\sigma^2
\end{matrix}\right)
\ea
Then the model can be written as
$$
\bm{Y}_i=\bm{\beta}+\bm{W}_i\bm{b}_i+\bm{\epsilon}_i
$$
The marginal variance/covariance matrix of $\bm{Y}_i$ is 
\ba
Var(\bm{Y}_i)&=Var(\bm{\beta}+\bm{W}_i\bm{b}_i+\bm{\epsilon}_i)=Var(\bm{W}_i\bm{b}_i)+Var(\bm{\epsilon}_i)\\
&=\bm{W}_iVar(\bm{b}_i)\bm{W}_i'+\bm{\Sigma}\\
&=\bm{W}_i\bm{D}\bm{W}_i'+\bm{\Sigma}\\
&=\left(\begin{matrix}
1&1\\
1&2
\end{matrix}\right)\left(\begin{matrix}
4&1\\
1&2
\end{matrix}\right)\left(\begin{matrix}
1&1\\
1&2
\end{matrix}\right)+\left(\begin{matrix}
2\\
&2
\end{matrix}\right)\\
&=\left(\begin{matrix}
10&11\\
11&18
\end{matrix}\right)
\ea

(b) The conditional variane/covariance matrix of $\bm{Y}_i$ is 
\ba
Var(\bm{Y}_i|\bm{b}_i)=Var(\bm{\epsilon}_i)=\bm{\Sigma}=\left(\begin{matrix}
2&0\\
0&2
\end{matrix}\right)\\
\ea

(c). The hypotheses are
$$
H_0:cov(b_{0i},b_{1i})=0\quad\text{v.s.}\quad H_a:cov(b_{0i},b_{1i})=0
$$
Then statistic is 
$$
\Lambda=-2(\ell(Reduced~Model)-\ell(Full~Model))=426-420=6\sim\chi^2(1)
$$
Then the value of statistic is greater than $\chi^2_{0.95}(1)=3.84$ with the P-value=0.014. Thus, we reject the null hypothesis, that is, we should favor the model for which the random effects parameters are dependent.



$\blacktriangleright$ \textbf{2.\quad Solution.} 
(a). We have the form of model 
$$
\bm{Y}=\bm{X}\bm{\beta}+\bm{Z}\bm{b}+\bm{\epsilon}
$$
where $
\bm{Y}=(y_1,y_2)',~
\bm{X}=(X_1,X_2)',~
\bm{\beta}=\beta,~
\bm{b}=(b_1,b_2)',~
\bm{\epsilon}=(\epsilon_1,\epsilon_2)',
$ and
\ba
\bm{Z}&=\left(\begin{matrix}
1\\
&2\end{matrix}\right)\\
\bm{D}&=\left(\begin{matrix}
\tau^2&\frac{\phi\tau^2}{1+\phi^2}\\
\frac{\phi\tau^2}{1+\phi^2}&\tau^2\end{matrix}\right)\\
\bm{\Sigma}&=\left(\begin{matrix}
\sigma^2\\
&\sigma^2\end{matrix}\right)
\ea


(b). The marginal variance/covariance matrix of $\bm{Y}$ is that
$$
Var(\bm{Y})=\bm{ZDZ}^T+\bm{\Sigma}=\left(\begin{matrix}
\tau^2+\sigma^2&\frac{2\phi\tau^2}{1+\phi^2}\\
\frac{2\phi\tau^2}{1+\phi^2}&4\tau^2+\sigma^2\end{matrix}\right)
$$
Then the marginal variance of $Y_2$ is $4\tau^2+\sigma^2$ and the marginal covariance between $Y_1$ and $Y_2$ is 
$$
cov(Y_1,~Y_2)=\frac{2\phi\tau^2}{1+\phi^2}
$$



(c). Nothing. Because in the restricted likelihood function there is no parameters other than those from variance and covariance matrix, we can only test the variances and covariances, but no the parameters of fixed and random effects based on REML. 


$\blacktriangleright$ \textbf{3.\quad Solution.} 
(a). The intercepts should be same among the different graphs, but the increments of different graphs should be different, and the time points of the maximum weights should be different.


(b). We have the form of model 
$$
\bm{Y}_i=\bm{X}_i\bm{\beta}+\bm{Z}_i\bm{b}_i+\bm{e}_i
$$
where 
\ba
\bm{Y}_i&=(y_{i1},\dots,y_{in_i})',\\
\bm{X}_i&=\left(\begin{matrix}
1&t_{i1}&t_{i1}^2\\
\vdots&\vdots&\vdots\\
1&t_{in_i}&t_{in_i}^2\end{matrix}\right),\\
\bm{Z}&=(t_{i1},\dots,t_{in_i})',\\
\bm{\beta}&=(\beta_0,\beta_1,\beta_2)',\\
\bm{b}_i&=b_{1i},\\
\bm{e}&=(e_1,\dots,e_{in_i})',\\
var(\bm{e}_i)&=\bm{\Sigma}=\left(\begin{matrix}
\sigma^2\\
&\ddots\\
&&\sigma^2\end{matrix}\right)=\sigma^2\bm{I}_{n_i\times n_i},\\
var(\bm{b}_i)&=\bm{D}=\left(\begin{matrix}
\sigma_b^2\\
&\ddots\\
&&\sigma_b^2\end{matrix}\right)=\sigma_b^2\bm{I}_{n_i\times n_i},\\
\ea


(c). The marginal variance/covariance matrix of $\bm{Y}$ is that
\ba
Var(\bm{Y}_i)&=\bm{Z}_i\bm{DZ}_i^T+\bm{\Sigma}\\
&=\left(\begin{matrix}
t_{i1}\\
\vdots\\
t_{in_i}\end{matrix}\right)
\left(\begin{matrix}
\sigma_b^2\\
&\ddots\\
&&\sigma_b^2\end{matrix}\right)
(t_{i1},\dots,t_{in_i})+\left(\begin{matrix}
\sigma^2\\
&\ddots\\
&&\sigma^2\end{matrix}\right)\\
&=\left(\begin{matrix}
t_{i1}^2\sigma_b^2+\sigma^2&t_{i1}t_{i2}\sigma_b^2&\cdots&t_{i1}t_{in_i}\sigma_b^2\\
t_{i2}t_{i1}\sigma_b^2&t_{i2}^2\sigma_b^2+\sigma^2&\cdots&t_{i2}t_{in_i}\sigma_b^2\\
\vdots&\vdots&\ddots&\vdots\\
t_{in_i}t_{i1}\sigma_b^2&t_{in_i}t_{i2}\sigma_b^2&\cdots&t_{in_i}^2\sigma_b^2+\sigma^2\\
\end{matrix}\right)
\ea


(d). Because the marginal covariance $\bm{Y}$ is
$$
cov(Y_{ij},Y_{ik})=t_{ij}t_{ik}\sigma_b^2
$$
then the correlation of $Y_i$ is that 
\ba
cov(Y_{ij},Y_{ik})&=\frac{cov(Y_{ij},Y_{ik})}{\sqrt{var(Y_{ij})}\sqrt{var(Y_{ik})}}\\
&=\frac{t_{ij}t_{ik}\sigma_b^2}{\sqrt{t_{ij}^2\sigma_b^2+\sigma^2}\sqrt{t_{ik}^2\sigma_b^2+\sigma^2}}\\
&=\frac{jk}{\sqrt{j^2+1}\sqrt{k^2+1}}\\
&=\frac{1}{\sqrt{1/j^2+1}\sqrt{1/k^2+1}}
\ea
The correlations will increase with the increase of time, $j$ and $k$, but no trend just with temporal separation. This is not so realistic. In common sense, we usually may think that the correlations may be smaller with large temporal separation than the correlations with small small temporal separation, because status of one time point is more likely to affect or to be affected by the status of the near time point. The reason causing this unrealistic result may be we simply assume the conditional independence while the data may not have this property.

(e). There are two advantages of the marginal covariance derived hierarchically. First, compared to the unstructured covariance structure, the hierarchical marginal covariance have less unknown parameters to estimate, so it can reduce the computation, and avoid suffering overfitting problem. Secondly, it easily to understand and interpret the variance components, we can know that which parts of variation come from random effect and which parts come from the violation of conditional independence.




$\blacktriangleright$ \textbf{4.\quad Solution.} 
(a) We have the split-plot design model
$$
Y_{ijk}=\mu+\rho_i+\alpha_j+e_{ij}+\beta_k+(\alpha\beta)_{jk}+\epsilon_{ijk}
$$
where $\rho$ is plot effect, $\alpha$ is pasture effect and $\beta$ is mineral effect. The random effects are $\rho$, $e$ and $\epsilon$. From the ANOVA table, we have that
$$
\sigma_{\text{plot}}^2=12.74,\ \ \sigma^2_{e}=1.05,\ \ \sigma_\epsilon^2=2.25
$$











%\Listing[store=class,
%  caption={Regression Analysis}]{resultl}

%\Graphic[store=class, scale=0.9,
%  caption={Graphs for Regression Analysis}]{resultg}

\end{document}
