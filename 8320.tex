\documentclass[letterpaper, 12pt]{article}


\usepackage{parskip,xspace}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{mathrsfs} 
\usepackage{caption}
\usepackage{xcolor} 
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{ltxtable}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{bm}
\usepackage[]{statrep}
\usepackage{enumerate}
\usepackage{subfigure}
\graphicspath{{eps/}}


\newcommand{\ba}{$$\begin{aligned}}
\newcommand{\ea}{\end{aligned}$$}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\lma}{\left(\begin{matrix}}
\newcommand{\rma}{\end{matrix}\right)}





\pagestyle{fancy}
\lhead{Peng Shao 14221765}
\chead{}
\rhead{\bfseries STAT 8320 Spring 2015 Assignment 4}
\renewcommand{\headrulewidth}{0.4 pt}
\setlength{\parindent}{2em}

\begin{document}
\title{STAT 8320 Spring 2015 Assignment 4}
\author{Peng Shao 14221765}
\maketitle
\indent




$\blacktriangleright$ \textbf{1.\quad Solution.} 
(a). Define
\ba
\bm{Y}_i&=(Y_i1,Y_i2)'\\
\bm{\beta}&=(\beta_0,\beta_0)'\\
\bm{b}_i&=(b_{0i},b_{1i})'\\
\bm{W}_i&=\left(\begin{matrix}
1&W_{i1}\\
1&W_{i2}
\end{matrix}\right)\\
\bm{\epsilon}_i&=(\epsilon_{i1},\epsilon_{i2})'\\
\bm{D}&=\left(\begin{matrix}
4 &1\\
1 &2
\end{matrix}\right)\\
\bm{\Sigma}&=\left(\begin{matrix}
\sigma^2\\
&\sigma^2
\end{matrix}\right)
\ea
Then the model can be written as
$$
\bm{Y}_i=\bm{\beta}+\bm{W}_i\bm{b}_i+\bm{\epsilon}_i
$$
The marginal variance/covariance matrix of $\bm{Y}_i$ is 
\ba
Var(\bm{Y}_i)&=Var(\bm{\beta}+\bm{W}_i\bm{b}_i+\bm{\epsilon}_i)=Var(\bm{W}_i\bm{b}_i)+Var(\bm{\epsilon}_i)\\
&=\bm{W}_iVar(\bm{b}_i)\bm{W}_i'+\bm{\Sigma}\\
&=\bm{W}_i\bm{D}\bm{W}_i'+\bm{\Sigma}\\
&=\left(\begin{matrix}
1&1\\
1&2
\end{matrix}\right)\left(\begin{matrix}
4&1\\
1&2
\end{matrix}\right)\left(\begin{matrix}
1&1\\
1&2
\end{matrix}\right)+\left(\begin{matrix}
2\\
&2
\end{matrix}\right)\\
&=\left(\begin{matrix}
10&11\\
11&18
\end{matrix}\right)
\ea

(b) The conditional variane/covariance matrix of $\bm{Y}_i$ is 
\ba
Var(\bm{Y}_i|\bm{b}_i)=Var(\bm{\epsilon}_i)=\bm{\Sigma}=\left(\begin{matrix}
2&0\\
0&2
\end{matrix}\right)\\
\ea

(c). The hypotheses are
$$
H_0:cov(b_{0i},b_{1i})=0\quad\text{v.s.}\quad H_a:cov(b_{0i},b_{1i})=0
$$
Then statistic is 
$$
\Lambda=-2(\ell(Reduced~Model)-\ell(Full~Model))=426-420=6\sim\chi^2(1)
$$
Then the value of statistic is greater than $\chi^2_{0.95}(1)=3.84$ with the P-value=0.014. Thus, we reject the null hypothesis, that is, we should favor the model for which the random effects parameters are dependent.



$\blacktriangleright$ \textbf{2.\quad Solution.} 
(a). We have the form of model 
$$
\bm{Y}=\bm{X}\bm{\beta}+\bm{Z}\bm{b}+\bm{\epsilon}
$$
where $
\bm{Y}=(y_1,y_2)',~
\bm{X}=(X_1,X_2)',~
\bm{\beta}=\beta,~
\bm{b}=(b_1,b_2)',~
\bm{\epsilon}=(\epsilon_1,\epsilon_2)',
$ and
\ba
\bm{Z}&=\left(\begin{matrix}
1\\
&2\end{matrix}\right)\\
\bm{D}&=\left(\begin{matrix}
\tau^2&\frac{\phi\tau^2}{1+\phi^2}\\
\frac{\phi\tau^2}{1+\phi^2}&\tau^2\end{matrix}\right)\\
\bm{\Sigma}&=\left(\begin{matrix}
\sigma^2\\
&\sigma^2\end{matrix}\right)
\ea


(b). The marginal variance/covariance matrix of $\bm{Y}$ is that
$$
Var(\bm{Y})=\bm{ZDZ}^T+\bm{\Sigma}=\left(\begin{matrix}
\tau^2+\sigma^2&\frac{2\phi\tau^2}{1+\phi^2}\\
\frac{2\phi\tau^2}{1+\phi^2}&4\tau^2+\sigma^2\end{matrix}\right)
$$
Then the marginal variance of $Y_2$ is $4\tau^2+\sigma^2$ and the marginal covariance between $Y_1$ and $Y_2$ is 
$$
cov(Y_1,~Y_2)=\frac{2\phi\tau^2}{1+\phi^2}
$$



(c). Nothing. Because in the restricted likelihood function there is no parameters other than those from variance and covariance matrix, we can only test the variances and covariances, but no the parameters of fixed and random effects based on REML. 


$\blacktriangleright$ \textbf{3.\quad Solution.} 
(a). The intercepts should be same among the different graphs, but the increments of different graphs should be different, and the time points of the maximum weights should be different.


(b). We have the form of model 
$$
\bm{Y}_i=\bm{X}_i\bm{\beta}+\bm{Z}_i\bm{b}_i+\bm{e}_i
$$
where 
\ba
\bm{Y}_i&=(y_{i1},\dots,y_{in_i})',\\
\bm{X}_i&=\left(\begin{matrix}
1&t_{i1}&t_{i1}^2\\
\vdots&\vdots&\vdots\\
1&t_{in_i}&t_{in_i}^2\end{matrix}\right),\\
\bm{Z}&=(t_{i1},\dots,t_{in_i})',\\
\bm{\beta}&=(\beta_0,\beta_1,\beta_2)',\\
\bm{b}_i&=b_{1i},\\
\bm{e}&=(e_1,\dots,e_{in_i})',\\
var(\bm{e}_i)&=\bm{\Sigma}=\left(\begin{matrix}
\sigma^2\\
&\ddots\\
&&\sigma^2\end{matrix}\right)=\sigma^2\bm{I}_{n_i\times n_i},\\
var(\bm{b}_i)&=\bm{D}=\left(\begin{matrix}
\sigma_b^2\\
&\ddots\\
&&\sigma_b^2\end{matrix}\right)=\sigma_b^2\bm{I}_{n_i\times n_i},\\
\ea


(c). The marginal variance/covariance matrix of $\bm{Y}$ is that
\ba
Var(\bm{Y}_i)&=\bm{Z}_i\bm{DZ}_i^T+\bm{\Sigma}\\
&=\left(\begin{matrix}
t_{i1}\\
\vdots\\
t_{in_i}\end{matrix}\right)
\left(\begin{matrix}
\sigma_b^2\\
&\ddots\\
&&\sigma_b^2\end{matrix}\right)
(t_{i1},\dots,t_{in_i})+\left(\begin{matrix}
\sigma^2\\
&\ddots\\
&&\sigma^2\end{matrix}\right)\\
&=\left(\begin{matrix}
t_{i1}^2\sigma_b^2+\sigma^2&t_{i1}t_{i2}\sigma_b^2&\cdots&t_{i1}t_{in_i}\sigma_b^2\\
t_{i2}t_{i1}\sigma_b^2&t_{i2}^2\sigma_b^2+\sigma^2&\cdots&t_{i2}t_{in_i}\sigma_b^2\\
\vdots&\vdots&\ddots&\vdots\\
t_{in_i}t_{i1}\sigma_b^2&t_{in_i}t_{i2}\sigma_b^2&\cdots&t_{in_i}^2\sigma_b^2+\sigma^2\\
\end{matrix}\right)
\ea


(d). Because the marginal covariance $\bm{Y}$ is
$$
cov(Y_{ij},Y_{ik})=t_{ij}t_{ik}\sigma_b^2
$$
then the correlation of $Y_i$ is that 
\ba
cov(Y_{ij},Y_{ik})&=\frac{cov(Y_{ij},Y_{ik})}{\sqrt{var(Y_{ij})}\sqrt{var(Y_{ik})}}\\
&=\frac{t_{ij}t_{ik}\sigma_b^2}{\sqrt{t_{ij}^2\sigma_b^2+\sigma^2}\sqrt{t_{ik}^2\sigma_b^2+\sigma^2}}\\
&=\frac{jk}{\sqrt{j^2+1}\sqrt{k^2+1}}\\
&=\frac{1}{\sqrt{1/j^2+1}\sqrt{1/k^2+1}}
\ea
The correlations will increase with the increase of time, $j$ and $k$, but no trend just with temporal separation. This is not so realistic. In common sense, we usually may think that the correlations may be smaller with large temporal separation than the correlations with small small temporal separation, because status of one time point is more likely to affect or to be affected by the status of the near time point. The reason causing this unrealistic result may be we simply assume the conditional independence while the data may not have this property.

(e). There are two advantages of the marginal covariance derived hierarchically. First, compared to the unstructured covariance structure, the hierarchical marginal covariance have less unknown parameters to estimate, so it can reduce the computation, and avoid suffering overfitting problem. Secondly, it easily to understand and interpret the variance components, we can know that which parts of variation come from random effect and which parts come from the violation of conditional independence.




$\blacktriangleright$ \textbf{4.\quad Solution.} 
(a) We have the split-plot design model
$$
Y_{ijk}=\mu+\rho_i+\alpha_j+e_{ij}+\beta_k+(\alpha\beta)_{jk}+\epsilon_{ijk}
$$
where $\rho$ is plot effect, $\alpha$ is pasture effect and $\beta$ is mineral effect. The random effects are $\rho$, $e$ and $\epsilon$. From the ANOVA table, we have that
$$
\sigma_{\text{plot}}^2=12.74,\ \ \sigma^2_{e}=1.05,\ \ \sigma_\epsilon^2=2.25
$$
In addition, the test for the significance for interaction of the pasture and mineral effects yields a P-value of 0.4981, the Factor pasture effect yields a P-value of 0.0377, and the Factor mineral effect yields a P-value of 0.0932. So only pasture effect are significant at $\alpha=0.05$.

(b).




(c). From the output "Differences of Least Squares Means", we can see that the only significance difference is the difference between pasture 1 and pasture 4 based on the Tukey-Kramer adjustment.


$\blacktriangleright$ \textbf{5.\quad Solution.}
(a). From the two graphs, we can see that there are a lot of distinct line, which indicate the difference of different subject, the random effects. The mean intercepts of the lines from different groups almost same, and they should be because the experimental subjects should be randomly assigned to one group and they all come from a same population. Furthermore, the trends of different groups are different show that there may be some interaction between group effect and visits. Finally, the score trends with respect to time seems to be linear, and two lack of fit tests show that it is reasonable assumption with P-values are 0.9127 and 0.9131.

To sum up, we should have 3 assumptions from the graphs: i) same intercept; 2) different slope; 3) linearity.





(b). Firstly, we will define some notations. We denote $i$ as the index of group with $i=0,1$; $j$ as the index of visit with $j=1,2,3,4,5$ and $k$ as the index of subject with $k=1,2,\cdots,47$. $\bm{Y}_{k}=(y_{i1k},y_{i2k},y_{i3k},y_{i4k},y_{i5k})'$ denotes the repeated responses of a single subject. $X_1$ is a indictor variable of lecithin group, where $X_{1k}=1$ when the subject in lecithin group. $X_2$ is a indictor variable of placebo group, where $X_{2k}=1$ when the subject in placebo group. $t$ is treated as a continuous variable of visit. $X_1t$  and $X_2t$ denote the interaction of group and visit, which will show the different slopes of different groups. $\beta_0$ is the identical intercept, $\beta_1$ and $\beta_2$ are the coefficients of $X_1t$  and $X_2t$. $b_{0k}$ is the random component of the intercept, where $b_{0k}\sim \text{N}(0,\sigma_b^2)$. $\epsilon_{ijk}$ is error term, and $\epsilon_{ijk}\sim \text{N}(0,\sigma^2)$ Now we can construct the model based on the general structure,
$$
\bm{Y}=\bm{X\beta+Zb+\epsilon}
$$
where the $\bm{X\beta}$ are fixed effects, and $\bm{Zb}$ are random effects, and 
\begin{itemize}
\item $\bm{Y}=(\bm{Y}_1',\bm{Y}_2',\dots,\bm{Y}_{47}')'$;
\item $\bm{X}=(\bm{1},\bm{X_1t},\bm{X_2t})=\lma
1&1&\cdots&1\\
X_{11}t&X_{12}t&\cdots&X_{1,47}t\\
X_{21}t&X_{22}t&\cdots&X_{2,47}t\rma'$;

\item $\bm{\beta}=(\beta_0,\beta_1,\beta_2)'$;
\item $\bm{Z}=\bm{1}=(1,1,\dots,1)'$;
\item $\bm{b}=b_0$;
\item $\bm{\epsilon}=(\epsilon_{1,1,1},\epsilon_{1,2,1},\dots,\epsilon_{1,5,25},\epsilon_{2,1,26},\dots,\epsilon_{2,5,47})'$.
\end{itemize}
We can also know that,
$$
var(\bm{b})=D=\big\{_d\{_d \sigma_b^2\}_{j=1}^5\big\}_{k=1}^{25},\qquad var(\bm{\epsilon})=\Sigma=\big\{_d\{_d \sigma^2\}_{j=1}^5\big\}_{k=1}^{25}
$$
where $\{_d \cdot\}$ denotes a diagonal matrix.

In this model, we assume that
\begin{itemize}
\item different groups have same intercept;
\item intercept has random component;
\item
\end{itemize}


%\Listing[store=class,
%  caption={Regression Analysis}]{resultl}

%\Graphic[store=class, scale=0.9,
%  caption={Graphs for Regression Analysis}]{resultg}

\end{document}
