\documentclass[letterpaper, 12pt]{article}


\usepackage{parskip,xspace}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{mathrsfs} 
\usepackage{caption}
\usepackage{xcolor} 
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{ltxtable}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{bm}
\usepackage[]{statrep}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage[toc,page]{appendix}

\graphicspath{{eps/}}


\newcommand{\ba}{$$\begin{aligned}}
\newcommand{\ea}{\end{aligned}$$}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\lma}{\left(\begin{matrix}}
\newcommand{\rma}{\end{matrix}\right)}




\pagestyle{fancy}
\lhead{Peng Shao 14221765}
\chead{}
\rhead{\bfseries STAT 8320 Spring 2015 Assignment 4}
\renewcommand{\headrulewidth}{0.4 pt}
\setlength{\parindent}{2em}

\begin{document}
\title{STAT 8320 Spring 2015 Assignment 4}
\author{Peng Shao 14221765}
\maketitle
\indent




$\blacktriangleright$ \textbf{1.\quad Solution.} 
(a). 
\ba
f(\lambda|y_i)&=\frac{f(y_i|\lambda)f(\lambda)}{f(y_i}\\
&=\frac{\frac{\lambda^{y_i+a-1}}{\Gamma(a)b^ay_i!}e^{-\lambda(1+1/b)}}{f(y_i)}\\
&\propto \frac{\lambda^{y_i+a-1}}{\Gamma(a)b^ay_i!}e^{-\lambda(1+1/b)}\\
&\propto \lambda^{y_i+a-1}e^{-\lambda(1+1/b)}
\ea
So $\lambda|y_i\sim\text{GAM}(y_i+a-1,\frac{1}{1+1/b})$, and 
$$
f(\lambda|y_i)=\frac{\lambda^{y_i+a-1}}{\Gamma(y_i+a)\left(\frac{b}{1+b}\right)^{y_i+a}}e^{-\lambda(1+1/b)}
$$
Thus, 
\ba
f(y_i)&=\int_0^\infty f(y_i|\lambda)f(\lambda)d\lambda=\frac{f(y_i|\lambda)f(\lambda)}{f(\lambda|y_i)}\\
&=\frac{\frac{\lambda^{y_i+a-1}}{\Gamma(a)b^ay_i!}e^{-\lambda(1+1/b)}}{\frac{\lambda^{y_i+a-1}}{\Gamma(y_i+a)\left(\frac{b}{1+b}\right)^{y_i+a}}e^{-\lambda(1+1/b)}}\\
&=\frac{\Gamma(y_i+a)}{\Gamma(a)y_i!}\left(\frac{1}{1+b}\right)^a\left(\frac{b}{1+b}\right)^{y_i}\\
&=\binom{a+y_i-1}{a-1}\left(\frac{b}{1+b}\right)^{y_i}\\
\ea
We can conclude that $y_i\sim\text{NB}(\frac{1}{1+b},a)$.






(b) From the theories in generalized linear model, we have already known that negative binomial distribution usually is used to fixed the over-dispersion problem of count data when Poisson distribution assumption or independence assumption are no longer valid. And we also know that in most time the over-dispersion may be caused by the dependence of data, like some repeated measurements in student attendance example. The GLMM essentially takes covariates between dependent data into model, so it also can model the over-dispersed count data. Or in other words, the derivation in part (a) just shows that negative binomial distribution can work well with over-dispersed count data.







$\blacktriangleright$ \textbf{2.\quad Solution.} 
(a). We have the form of model 
$$
\bm{Y}=\bm{X}\bm{\beta}+\bm{Z}\bm{b}+\bm{\epsilon}
$$
where $
\bm{Y}=(y_1,y_2)',~
\bm{X}=(X_1,X_2)',~
\bm{\beta}=\beta,~
\bm{b}=(b_1,b_2)',~
\bm{\epsilon}=(\epsilon_1,\epsilon_2)',
$ and
\ba
\bm{Z}&=\left(\begin{matrix}
1\\
&2\end{matrix}\right)\\
\bm{D}&=\left(\begin{matrix}
\tau^2&\frac{\phi\tau^2}{1+\phi^2}\\
\frac{\phi\tau^2}{1+\phi^2}&\tau^2\end{matrix}\right)\\
\bm{\Sigma}&=\left(\begin{matrix}
\sigma^2\\
&\sigma^2\end{matrix}\right)
\ea


(b). The marginal variance/covariance matrix of $\bm{Y}$ is that
$$
Var(\bm{Y})=\bm{ZDZ}^T+\bm{\Sigma}=\left(\begin{matrix}
\tau^2+\sigma^2&\frac{2\phi\tau^2}{1+\phi^2}\\
\frac{2\phi\tau^2}{1+\phi^2}&4\tau^2+\sigma^2\end{matrix}\right)
$$
Then the marginal variance of $Y_2$ is $4\tau^2+\sigma^2$ and the marginal covariance between $Y_1$ and $Y_2$ is 
$$
cov(Y_1,~Y_2)=\frac{2\phi\tau^2}{1+\phi^2}
$$



(c). Nothing. Because in the restricted likelihood function there is no parameters other than those from variance and covariance matrix, we can only test the variances and covariances, but no the parameters of fixed and random effects based on REML. 


$\blacktriangleright$ \textbf{3.\quad Solution.} 
(a). If the intercepts of eight plots are exactly at same point, but the increments are more complicated and a little fluctuant, not just a simple quadratic curve. Then we should consider adding a random component into the coefficient for time.

(b). We have the form of model 
$$
\bm{Y}_i=\bm{X}_i\bm{\beta}+\bm{Z}_i\bm{b}_i+\bm{e}_i
$$
where 
\ba
\bm{Y}_i&=(y_{i1},\dots,y_{in_i})',\\
\bm{X}_i&=\left(\begin{matrix}
1&t_{i1}&t_{i1}^2\\
\vdots&\vdots&\vdots\\
1&t_{in_i}&t_{in_i}^2\end{matrix}\right),\\
\bm{Z}&=(t_{i1},\dots,t_{in_i})',\\
\bm{\beta}&=(\beta_0,\beta_1,\beta_2)',\\
\bm{b}_i&=b_{1i},\\
\bm{e}&=(e_{11},\dots,e_{in_i})',\\
var(\bm{e}_i)&=\bm{\Sigma}=\left(\begin{matrix}
\sigma^2\\
&\ddots\\
&&\sigma^2\end{matrix}\right)=\sigma^2\bm{I}_{n_i\times n_i},\\
var(\bm{b}_i)&=\bm{D}=\left(\begin{matrix}
\sigma_b^2\\
&\ddots\\
&&\sigma_b^2\end{matrix}\right)=\sigma_b^2\bm{I}_{n_i\times n_i},\\
\ea


(c). The marginal variance/covariance matrix of $\bm{Y}$ is that
\ba
Var(\bm{Y}_i)&=\bm{Z}_i\bm{DZ}_i^T+\bm{\Sigma}\\
&=\left(\begin{matrix}
t_{i1}\\
\vdots\\
t_{in_i}\end{matrix}\right)
\left(\begin{matrix}
\sigma_b^2\\
&\ddots\\
&&\sigma_b^2\end{matrix}\right)
(t_{i1},\dots,t_{in_i})+\left(\begin{matrix}
\sigma^2\\
&\ddots\\
&&\sigma^2\end{matrix}\right)\\
&=\left(\begin{matrix}
t_{i1}^2\sigma_b^2+\sigma^2&t_{i1}t_{i2}\sigma_b^2&\cdots&t_{i1}t_{in_i}\sigma_b^2\\
t_{i2}t_{i1}\sigma_b^2&t_{i2}^2\sigma_b^2+\sigma^2&\cdots&t_{i2}t_{in_i}\sigma_b^2\\
\vdots&\vdots&\ddots&\vdots\\
t_{in_i}t_{i1}\sigma_b^2&t_{in_i}t_{i2}\sigma_b^2&\cdots&t_{in_i}^2\sigma_b^2+\sigma^2\\
\end{matrix}\right)_{n_i\times n_i}
\ea


(d). Because the marginal covariance $\bm{Y}$ is
$$
cov(Y_{ij},Y_{ik})=t_{ij}t_{ik}\sigma_b^2
$$
then the correlation of $Y_i$ is that 
\ba
corr(Y_{ij},Y_{ik})&=\frac{cov(Y_{ij},Y_{ik})}{\sqrt{var(Y_{ij})}\sqrt{var(Y_{ik})}}\\
&=\frac{t_{ij}t_{ik}\sigma_b^2}{\sqrt{t_{ij}^2\sigma_b^2+\sigma^2}\sqrt{t_{ik}^2\sigma_b^2+\sigma^2}}\\
&=\frac{jk}{\sqrt{j^2+1}\sqrt{k^2+1}}\\
&=\frac{1}{\sqrt{1/j^2+1}\sqrt{1/k^2+1}}
\ea
The correlations will increase with the increase of time, $j$ and $k$, but no trend just with temporal separation. This is not so realistic. In common sense, we usually may think that the correlations may be smaller with large temporal separation than the correlations with small small temporal separation, because status of one time point is more likely to affect or to be affected by the status of the near time point. The reason causing this unrealistic result may be we simply assume the conditional independence while the data may not have this property.

(e). There are two advantages of the marginal covariance derived hierarchically. First, compared to the unstructured covariance structure, the hierarchical marginal covariance have less unknown parameters to estimate, so it can reduce the computation, and avoid suffering overfitting problem. Secondly, it easily to understand and interpret the variance components, we can know that which parts of variation come from random effect and which parts come from the violation of conditional independence.




$\blacktriangleright$ \textbf{4.\quad Solution.} 
(a) We have the split-plot design model
$$
Y_{ijk}=\mu+\rho_i+\alpha_j+e_{ij}+\beta_k+(\alpha\beta)_{jk}+\epsilon_{ijk}
$$
where $\rho$ is plot effect, $\alpha$ is pasture effect and $\beta$ is mineral effect. The random effects are $\rho$, $e$ and $\epsilon$. From the ANOVA table(Figure \ref{covest}, Figure \ref{anova}), we have that
$$
\sigma_{\text{plot}}^2=12.74,\ \ \sigma^2_{e}=1.05,\ \ \sigma_\epsilon^2=2.25
$$
In addition, the test(Figure \ref{h4re10}) for the significance for interaction of the pasture and mineral effects yields a P-value of 0.4981, the Factor pasture effect yields a P-value of 0.0377, and the Factor mineral effect yields a P-value of 0.0932. So only pasture effect are significant at $\alpha=0.05$.
\Listing[store=class,caption={Anova Table}]{anova}
\Listing[store=class,caption={Covariance Estimates}]{covest}
\Listing[store=class,caption={Type 3 Tests}]{h4re10}


(b). Because the data may come from different distribution, so the degree of freedom of the variance of random components may need some modification, like Satterthwaite method. The Kenwardroger method give us a more conservative distribution about t-test or F-test than Satterthwaite when sample size is not large enough, making the assumption seem more appropriate. For example, we assume that $E(\rho)$=0. When we test whether every predict of $\rho_i$ equals zero. For unmodified degree of freedom, the degree of freedom of t-statistic for $\rho_1$ is 8 and the P-value is 0.0965, but for Kenwardroger method, the degree of freedom is 2.14 and the P-value is 0.1934. The results of not rejecting null hypothesis are same, but which of Kenwardroger method makes us a little harder to reject our assumption.

The method only modified the degree of freedom, so it will leave the result of fixed result unchanged. In this specific problem, the results of random components are also unchanged.

(c). From the output "Differences of Least Squares Means" (Figure \ref{h4re23}), we can see that the only significance difference is the difference between pasture 1 and pasture 4 based on the Tukey-Kramer adjustment.
\Listing[store=class,caption={Least Squares Means}]{h4re23}

$\blacktriangleright$ \textbf{5.\quad Solution.}
(a). The hypotheses are
$$
H_0:\mu_{11}=\mu_{12}=\mu_{13}\qquad\text{v.s.}\qquad H_a: \text{ at least two means are not equal}
$$
or we can write null hypothesis as
$$
H_0: \bm{C}_1\bm{\mu}_1=\bm{0}
$$
where
$$
\bm{C}_1=\lma 1 &-1 &0\\ 0 &1 &-1\rma
$$
The statistics are


\begin{appendices}
\section{SAS}
\section{Output}

\end{appendices}







%\Listing[store=class,
%  caption={Regression Analysis}]{resultl}

%\Graphic[store=class, scale=0.9,
%  caption={Graphs for Regression Analysis}]{resultg}

\end{document}
