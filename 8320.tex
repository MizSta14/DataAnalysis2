\documentclass[letterpaper, 12pt]{article}


\usepackage{parskip,xspace}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{mathrsfs} 
\usepackage{caption}
\usepackage{xcolor} 
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{ltxtable}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{bm}
\usepackage[]{statrep}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage[toc,page]{appendix}

\graphicspath{{eps/}}


\newcommand{\ba}{$$\begin{aligned}}
\newcommand{\ea}{\end{aligned}$$}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\lma}{\left(\begin{matrix}}
\newcommand{\rma}{\end{matrix}\right)}




\pagestyle{fancy}
\lhead{Peng Shao 14221765}
\chead{}
\rhead{\bfseries STAT 8320 Spring 2015 Assignment 5}
\renewcommand{\headrulewidth}{0.4 pt}
\setlength{\parindent}{2em}

\begin{document}
\title{STAT 8320 Spring 2015 Assignment 5}
\author{Peng Shao 14221765}
\maketitle
\indent




$\blacktriangleright$ \textbf{1.\quad Solution.} 
(a). 
\ba
f(\lambda|y_i)&=\frac{f(y_i|\lambda)f(\lambda)}{f(y_i}\\
&=\frac{\frac{\lambda^{y_i+a-1}}{\Gamma(a)b^ay_i!}e^{-\lambda(1+1/b)}}{f(y_i)}\\
&\propto \frac{\lambda^{y_i+a-1}}{\Gamma(a)b^ay_i!}e^{-\lambda(1+1/b)}\\
&\propto \lambda^{y_i+a-1}e^{-\lambda(1+1/b)}
\ea
So $\lambda|y_i\sim\text{GAM}(y_i+a-1,\frac{1}{1+1/b})$, and 
$$
f(\lambda|y_i)=\frac{\lambda^{y_i+a-1}}{\Gamma(y_i+a)\left(\frac{b}{1+b}\right)^{y_i+a}}e^{-\lambda(1+1/b)}
$$
Thus, 
\ba
f(y_i)&=\int_0^\infty f(y_i|\lambda)f(\lambda)d\lambda=\frac{f(y_i|\lambda)f(\lambda)}{f(\lambda|y_i)}\\
&=\frac{\frac{\lambda^{y_i+a-1}}{\Gamma(a)b^ay_i!}e^{-\lambda(1+1/b)}}{\frac{\lambda^{y_i+a-1}}{\Gamma(y_i+a)\left(\frac{b}{1+b}\right)^{y_i+a}}e^{-\lambda(1+1/b)}}\\
&=\frac{\Gamma(y_i+a)}{\Gamma(a)y_i!}\left(\frac{1}{1+b}\right)^a\left(\frac{b}{1+b}\right)^{y_i}\\
&=\binom{a+y_i-1}{a-1}\left(\frac{b}{1+b}\right)^{y_i}\\
\ea
We can conclude that $y_i\sim\text{NB}(\frac{1}{1+b},a)$.






(b) From the theories in generalized linear model, we have already known that negative binomial distribution usually is used to fixed the over-dispersion problem of count data when Poisson distribution assumption or independence assumption are no longer valid. And we also know that in most time the over-dispersion may be caused by the dependence of data, like some repeated measurements in student attendance example. The GLMM essentially takes covariates between dependent data into model, so it also can model the over-dispersed count data. Or in other words, the derivation in part (a) just shows that negative binomial distribution can work well with over-dispersed count data.







$\blacktriangleright$ \textbf{2.\quad Solution.} 
(a). Before we fit this nonlinear mixed model, we should firstly plot the profile of the data, trying to acquire some intuitive result from the plot. As the Figure \ref{profile} shows, different plants are label as 1 to 6, and number 7 represents the average profile. We can approximately know that the max value is between 150 and 250, and the inflection point should be between 500 and 1000. Without loss of generality, we set the initial value of $(\beta_1,\beta_2)$ as (200, 850). Then we can solve the $\beta_3$ based on the data, and it is about 350. Next, by using PROC MEANS in SAS, we can get the standard deviations at different time points. Because $1+e^{-(t_{ij}-\beta_2)/\beta_3}$ is relatively large, it is reasonable to assume that the variance of $Y$ is mostly from $\sigma^2$. So we set the initial value of $\sigma^2$ as 40. As $t_{ij}$ grows, the denominator becomes smaller and smaller, then the proportion of $\sigma_u^2$ in variance of $Y$ becomes larger, and it seems that $\sigma_u^2$ should be between 400 to 1600, so we set the initial value of $\sigma_u^2$ as 900.
\Graphic[store=class, scale=0.5,
         caption={Profile of Plant Growth}]{profile}


Then we use the PROC NLIN and PROC NLMIXED to fit the nonlinear model and the nonlinear mixed model respectively. The results about parameters are listed as Figure \ref{nlinpara} and Figure \ref{h5re11}.
\Listing[store=class,
         caption={Parameters of Nonlinear Models}]{nlinpara}
\Listing[store=class,
         caption={Parameters of Nonlinear Mixed Models}]{h5re11}

From these output, we can answer the questions like,
\begin{enumerate}
\item To test 
$$
H_0:\beta_3=350\qquad\text{v.s.}\qquad H_A:\beta_3\not=350
$$
we can easily reject the null hypothesis because the Wald-type confidence intervals of both models do not contain 350, which is equivalent to a Wald test. 
We also can use the ESTIMATE statement in PROC NLMIXED to estimate $\beta_3-350$, as Figure \ref{h5re12}
\Listing[store=class,
         caption={Additional Estimates}]{h5re12}
It is obviously that we should reject the null hypothesis, which is the same result as above. So $\beta_3$ does not equal 350.
\item To test whether the random effect is necessary. Because the method of parameter estimate of mixed model is not based on likelihood, we cannot use likelihood ratio test. So we still use the Wald-type confidence interval. Because the interval contains zero, we cannot reject the null hypothesis, that is, the random effect is not significant. Furthermore, we can see that the estimate of parameters of fixed effect does not change too much, so this also indicates that it is not necessary to introduce random effect into model.
\end{enumerate}

But, we should be cautious about the result, because from the parameter estimates of parameter of nonlinear model, skewness of all parameters is much greater than 0.25, which means all parameters have vary apparent skewness. This makes the inferences unreliable.


$\blacktriangleright$ \textbf{3.\quad Solution.} 
(a). If the intercepts of eight plots are exactly at same point, but the increments are more complicated and a little fluctuant, not just a simple quadratic curve. Then we should consider adding a random component into the coefficient for time.

(b). We have the form of model 
$$
\bm{Y}_i=\bm{X}_i\bm{\beta}+\bm{Z}_i\bm{b}_i+\bm{e}_i
$$
where 
\ba
\bm{Y}_i&=(y_{i1},\dots,y_{in_i})',\\
\bm{X}_i&=\left(\begin{matrix}
1&t_{i1}&t_{i1}^2\\
\vdots&\vdots&\vdots\\
1&t_{in_i}&t_{in_i}^2\end{matrix}\right),\\
\bm{Z}&=(t_{i1},\dots,t_{in_i})',\\
\bm{\beta}&=(\beta_0,\beta_1,\beta_2)',\\
\bm{b}_i&=b_{1i},\\
\bm{e}&=(e_{11},\dots,e_{in_i})',\\
var(\bm{e}_i)&=\bm{\Sigma}=\left(\begin{matrix}
\sigma^2\\
&\ddots\\
&&\sigma^2\end{matrix}\right)=\sigma^2\bm{I}_{n_i\times n_i},\\
var(\bm{b}_i)&=\bm{D}=\left(\begin{matrix}
\sigma_b^2\\
&\ddots\\
&&\sigma_b^2\end{matrix}\right)=\sigma_b^2\bm{I}_{n_i\times n_i},\\
\ea


(c). The marginal variance/covariance matrix of $\bm{Y}$ is that
\ba
Var(\bm{Y}_i)&=\bm{Z}_i\bm{DZ}_i^T+\bm{\Sigma}\\
&=\left(\begin{matrix}
t_{i1}\\
\vdots\\
t_{in_i}\end{matrix}\right)
\left(\begin{matrix}
\sigma_b^2\\
&\ddots\\
&&\sigma_b^2\end{matrix}\right)
(t_{i1},\dots,t_{in_i})+\left(\begin{matrix}
\sigma^2\\
&\ddots\\
&&\sigma^2\end{matrix}\right)\\
&=\left(\begin{matrix}
t_{i1}^2\sigma_b^2+\sigma^2&t_{i1}t_{i2}\sigma_b^2&\cdots&t_{i1}t_{in_i}\sigma_b^2\\
t_{i2}t_{i1}\sigma_b^2&t_{i2}^2\sigma_b^2+\sigma^2&\cdots&t_{i2}t_{in_i}\sigma_b^2\\
\vdots&\vdots&\ddots&\vdots\\
t_{in_i}t_{i1}\sigma_b^2&t_{in_i}t_{i2}\sigma_b^2&\cdots&t_{in_i}^2\sigma_b^2+\sigma^2\\
\end{matrix}\right)_{n_i\times n_i}
\ea


(d). Because the marginal covariance $\bm{Y}$ is
$$
cov(Y_{ij},Y_{ik})=t_{ij}t_{ik}\sigma_b^2
$$
then the correlation of $Y_i$ is that 
\ba
corr(Y_{ij},Y_{ik})&=\frac{cov(Y_{ij},Y_{ik})}{\sqrt{var(Y_{ij})}\sqrt{var(Y_{ik})}}\\
&=\frac{t_{ij}t_{ik}\sigma_b^2}{\sqrt{t_{ij}^2\sigma_b^2+\sigma^2}\sqrt{t_{ik}^2\sigma_b^2+\sigma^2}}\\
&=\frac{jk}{\sqrt{j^2+1}\sqrt{k^2+1}}\\
&=\frac{1}{\sqrt{1/j^2+1}\sqrt{1/k^2+1}}
\ea
The correlations will increase with the increase of time, $j$ and $k$, but no trend just with temporal separation. This is not so realistic. In common sense, we usually may think that the correlations may be smaller with large temporal separation than the correlations with small small temporal separation, because status of one time point is more likely to affect or to be affected by the status of the near time point. The reason causing this unrealistic result may be we simply assume the conditional independence while the data may not have this property.

(e). There are two advantages of the marginal covariance derived hierarchically. First, compared to the unstructured covariance structure, the hierarchical marginal covariance have less unknown parameters to estimate, so it can reduce the computation, and avoid suffering overfitting problem. Secondly, it easily to understand and interpret the variance components, we can know that which parts of variation come from random effect and which parts come from the violation of conditional independence.




$\blacktriangleright$ \textbf{4.\quad Solution.} 
(a) 
$$
\bm{Y}=\lma X_1\\X_3\rma\sim \text{N}\left(\lma -3 \\2\rma,\lma 4&0\\0&3\rma\right)
$$

(b)
\ba
\bm{Y}|X_2&\sim\text{N}\left(\lma -3\\2\rma+\lma \frac{x_2}{2}-\frac12\\\frac{x_2}2-\frac12\rma,\lma 3.5 &-0.5\\-0.5 &2.5\rma\right)\\
&=\text{N}\left(\lma \frac{x_2}{2}-\frac72\\\frac{x_2}2+\frac32\rma,\lma 3.5 &-0.5\\-0.5 &2.5\rma\right)
\ea


(c)
\ba
X_2|\bm{Y}&\sim\text{N}\left(2+\lma 1&1\rma\lma \frac14&0\\0&\frac13\rma\lma x_1+3\\x_3-2\rma, 2-\lma 1&1\rma\lma \frac14&0\\0&\frac13\rma\lma1\\1\rma\right)\\
&=\text{N}\left(\frac14x_1+\frac13x_3+\frac{13}{12},\frac{17}{12}\right).
\ea

(d) 
$$
Z\sim\text{N}(-3+3\cdot1,4+3^3\cdot2+2\cdot3\cdot1)=\text{N}(0,28)
$$

$\blacktriangleright$ \textbf{5.\quad Solution.}
(a). The hypotheses are
$$
H_0:\mu_{11}=\mu_{12}=\mu_{13}\qquad\text{v.s.}\qquad H_a: \text{ at least two means are not equal}
$$
or we can write null hypothesis as
$$
H_0: \bm{C}_1\bm{\mu}_1=\bm{0}
$$
where
$$
\bm{C}_1=\lma 1 &-1 &0\\ 0 &1 &-1\rma
$$
The statistics are
\ba
T^2&=n_1(\bm{C\bar{y}})'(\bm{CSC}')^{-1}(\bm{C\bar{y}})=111.4286\\
F&=\frac{n_1-c}{(n_1-1)c}T^2=53.3929\sim f_{c,n_1-c}
\ea
where $c=2$ and $n_1=25$. The critical value of $F$ statistic is $f_{0.95,2,23}=3.422$, so $F>f_{0.95,2,23}$ and P-value is $2.28\text{E}^{-09}$. We will reject the  null hypothesis, that is, the mean concentrations are significantly different at three time points.


(b). \begin{enumerate}[i]
\item The common covariance is
$$
\bm{S}_{pool}=\frac{(n_1-1)\bm{S}+(n_2-1)\bm{W}}{(n1-1)+(n_2-1)}=\lma 28.4 &10.8 & 12.4\\10.8 &15.8 &5.6\\12.4 & 5.6 &39.4\rma
$$
where $n_2=17$. The degree of freedom is 25+17-2=40.
\item The hypotheses are 
$$
H_0:\bm{\Sigma}_A=\bm{\Sigma}_B\qquad\text{v.s.}\qquad H_A:\bm{\Sigma}_A\not=\bm{\Sigma}_B
$$
The statistic is
\ba
M&=(n_1+n_2-2)\log|\bm{S}_{pool}|-(n_1-1)\log|\bm{S}|-(n_2-1)\log|\bm{W}|=1.1948\\
C^{-1}&=1-\frac{2\times3^2+3\times3-1}{6\times(3+1)\times(2-1)}\left\{\frac{1}{n_1-1}+\frac{1}{n_2-1}-\frac{1}{n_1+n_2-2}\right\}=0.9142\\
MC^{-1}&=1.1948\times0.9142=1.0923\sim\chi^2_{6}
\ea

Because $MC^{-1}=0.9142<\chi^2_{0.95,6}=12.592$ and P-value is 0.9819, we cannot reject the null hypothesis, which means that the assumption of same population covariance are reliable.

\item The squared Mahalanobis distance between $\bm{y}-\bm{z}$ is
$$
T^2=(\bm{y}-\bm{z})'\left[\bm{S}_{pool}\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\right]^{-1}(\bm{y}-\bm{z})=18.03155
$$


\item The hypotheses are
$$
H_0:\bm{y}-\bm{z}=\bm{0}\qquad\text{v.s.}\qquad H_A:\bm{y}-\bm{z}=\bm{0}
$$
The statistic can be computed from the Mahalanobis distance from part (iii)
$$
F=\frac{n_1+n_2-3-1}{(n_1+n_2-2)\times3}T^2=5.70999\sim f_{3,38}
$$
Because $F=5.70999>f_{0.95,3,38}=2.851$ with P-value=0.0025. We will reject the null hypothesis, so drug A and drug B do not have equal means.
\item To test the parallel profiles, the hypotheses are
$$
H_0:\mu_{11}-\mu_{21}=\mu_{12}-\mu_{22}=\mu_{13}-\mu_{23}\quad\text{v.s.}\quad H_A: \text{ at least two difference not equal}
$$
or we can rewrite the null hypothesis as
$$
H_0:\bm{C_2}(\bm{y}-\bm{z})=0
$$
where $\bm{C_2}=\lma 1 &-1 &0\\ 0 &1 &-1\rma$. 

The statistics are
\ba
T^2&=\frac{n_1n_2}{n_1+n_2}(\bm{C(\bar{y}-\bar{z})})'(\bm{CSC}')^{-1}(\bm{C(\bar{y}-\bar{z})})=14.2658\\
F&=\frac{n_1+n_2-c-1}{(n_1+n_2-2)c}T^2=6.7849\sim f_{c,n_1+n_2-c-1}
\ea
Because $F=6.7849>f_{0.95,2,39}=3.238$ with P-value=0.0030. We will reject the null hypothesis, so there is a significant interaction between drug and time.



\end{enumerate}













\begin{appendices}
\section{SAS}
\section{Output}

\end{appendices}







%\Listing[store=class,
%  caption={Regression Analysis}]{resultl}

%\Graphic[store=class, scale=0.9,
%  caption={Graphs for Regression Analysis}]{resultg}

\end{document}
